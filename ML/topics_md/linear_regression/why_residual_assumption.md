# why_residual_assumption

ğŸ’¥ Boom â€” youâ€™ve landed on another critical question. This is where **math, modeling, and inference all come together**. Let's answer it **clearly and precisely**.

---

## âœ… Two-part question:

1. **Why donâ€™t we directly assume that $y$ is normally distributed in OLS?**
2. **Are error and residual the same?**

---

### ğŸ¯ **1. Why donâ€™t we assume $y \sim \mathcal{N}(\mu, \sigma^2)$ directly in OLS?**

#### Because:

OLS isn't about modeling $y$ directly â€” it's about modeling **how $y$ depends on $x$**.

OLS assumes:

$$
y = X\beta + \varepsilon, \quad \text{with } \varepsilon \sim \mathcal{N}(0, \sigma^2)
$$

This is more precise than:

$$
y \sim \mathcal{N}(\mu, \sigma^2)
$$

Why?

* That second form loses **conditional dependence** of $y$ on $X$.
* We donâ€™t believe $y$ has the same mean for all inputs â€” we believe the mean depends on $x$:

  $$
  \mathbb{E}[y \mid x] = x^\top \beta
  $$

So instead of assuming:

> â€œy is normalâ€
> We assume:
> â€œThe **noise** around the regression line is normalâ€

That gives us:

* A **more flexible**, **conditional** model:

  $$
  y_i \mid x_i \sim \mathcal{N}(x_i^\top \beta, \sigma^2)
  $$

---

### ğŸ’¡ Summary:

> We donâ€™t model **y as globally normal**
> We model **y as normal *given x*** â†’ because regression is a conditional model

---

### ğŸ§  Think of this like:

> We donâ€™t say â€œexam scores are normalâ€
> We say â€œexam scores are normal *given how much you studied*â€

---

### ğŸ“Œ Now for Part 2:

## âœ… Are **error** and **residual** the same?

**Not quite.** They're related â€” but **not identical**.

| Term         | Symbol                                | What it is                              | Known/Unknown |
| ------------ | ------------------------------------- | --------------------------------------- | ------------- |
| **Error**    | $\varepsilon_i = y_i - x_i^\top\beta$ | True difference (uses true $\beta$)     | **Unknown**   |
| **Residual** | $e_i = y_i - x_i^\top\hat\beta$       | Estimated difference (uses $\hat\beta$) | **Known**     |

---

### ğŸ” Relation:

Residual $e_i$ is an **observable proxy** for the true error $\varepsilon_i$.
We use residuals to check if our model is behaving well (e.g., homoscedasticity, normality).

---

### âœ… Final Answers:

1. **We donâ€™t assume $y \sim \mathcal{N}$** because we're doing **conditional modeling** â†’ we assume $\varepsilon \sim \mathcal{N}$ instead.
2. **Error** is the true unknown deviation, **residual** is the observed estimate of that error after fitting the model.

---

Would you like a little code + plot to show how errors vs residuals behave differently on real data?
